import io
import threading
from queue import Queue
import torch
import torchaudio
from vad_utils import load_model, VADIterator
from transcriber import transcribe_audio
from config import voice_activity_threshold, min_silence_duration, min_speech_duration

# Initialize Audio Stream Queues
input_queue = Queue()

# Initialize Threading Events
speech_stop_event = threading.Event()

# Load Voice Activity Model
model = load_model()
vad_iterator = VADIterator(model, threshold=voice_activity_threshold, min_silence_duration_ms=min_silence_duration)

# Extracts speech from the audio buffer (initialized in process_audio) 
# using timestamps generated by VAD
def extract_audio(timestamps, wav: torch.Tensor):
    chunks = []
    for entry in timestamps:
        if "start" in entry:
            start = entry["start"]
        elif "end" in entry and start is not None:
            chunks.append(wav[start: entry["end"]])
    return torch.cat(chunks)

# Generates in-memory WAV audio file as a BytesIO object
def generate_wav(tensor: torch.Tensor):
    bytes = io.BytesIO()
    torchaudio.save(
        bytes,
        tensor.unsqueeze(0),
        16000,
        format = "wav",          
        encoding = "PCM_S",
        bits_per_sample = 16
    )
    bytes.name = "audio.wav"
    bytes.seek(0)
    return bytes

# Processes audio data from the Audio Stream Queue using VAD
def process_audio():
    # Initialize Buffers
    audio_buffer = torch.tensor([], dtype=torch.float32)
    vad_outputs = []

    while True:
        # Retrieve next chunk from queue -> Convert to tensor -> Add to Buffer
        audio_chunk = input_queue.get()
        audio_tensor = torch.from_numpy(audio_chunk.squeeze())
        audio_buffer = torch.cat((audio_buffer, audio_tensor))
        
        # Inference chunk
        vad_output = vad_iterator(audio_tensor)
        if vad_output is not None and len(vad_output) > 0:
            print(vad_output)
            # Set thread stop_event to stop currently generating speech
            speech_stop_event.set()

            vad_outputs.append(vad_output)
            if "end" in vad_outputs[-1]:
                # Moves start sample back by 3200 samples (0.2 seconds) 
                # to capture any speech that started slightly earlier.
                for i in range(len(vad_outputs)):
                    if "start" in vad_outputs[i]:
                        vad_outputs[i]["start"] = max(0, vad_outputs[i]["start"] - 3200)

                output_tensor = extract_audio(vad_outputs, audio_buffer)
                # If detected speech segment > than min_speech_duration then proceed
                if ((output_tensor.shape[0]/16000) * 1000) > min_speech_duration:
                    audio_bytes = generate_wav(output_tensor)

                    # Reset Buffers and iterator
                    audio_buffer = torch.tensor([], dtype=torch.float32)
                    vad_outputs = []
                    vad_iterator.reset_states()

                    # Make API Call to Groq in a different thread for transcription
                    speech_stop_event.clear()
                    transcription_thread = threading.Thread(target=transcribe_audio, args=(audio_bytes, speech_stop_event), daemon=True)
                    transcription_thread.start()
